{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "from tqdm import tqdm\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encounter_id</th>\n",
       "      <th>patient_nbr</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>weight</th>\n",
       "      <th>admission_type_id</th>\n",
       "      <th>discharge_disposition_id</th>\n",
       "      <th>admission_source_id</th>\n",
       "      <th>time_in_hospital</th>\n",
       "      <th>...</th>\n",
       "      <th>citoglipton</th>\n",
       "      <th>insulin</th>\n",
       "      <th>glyburide-metformin</th>\n",
       "      <th>glipizide-metformin</th>\n",
       "      <th>glimepiride-pioglitazone</th>\n",
       "      <th>metformin-rosiglitazone</th>\n",
       "      <th>metformin-pioglitazone</th>\n",
       "      <th>change</th>\n",
       "      <th>diabetesMed</th>\n",
       "      <th>readmitted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2278392</td>\n",
       "      <td>8222157</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Female</td>\n",
       "      <td>[0-10)</td>\n",
       "      <td>?</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>149190</td>\n",
       "      <td>55629189</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Female</td>\n",
       "      <td>[10-20)</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Up</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Ch</td>\n",
       "      <td>Yes</td>\n",
       "      <td>&gt;30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64410</td>\n",
       "      <td>86047875</td>\n",
       "      <td>AfricanAmerican</td>\n",
       "      <td>Female</td>\n",
       "      <td>[20-30)</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>500364</td>\n",
       "      <td>82442376</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Male</td>\n",
       "      <td>[30-40)</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Up</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Ch</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16680</td>\n",
       "      <td>42519267</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Male</td>\n",
       "      <td>[40-50)</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Steady</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Ch</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   encounter_id  patient_nbr             race  gender      age weight  \\\n",
       "0       2278392      8222157        Caucasian  Female   [0-10)      ?   \n",
       "1        149190     55629189        Caucasian  Female  [10-20)      ?   \n",
       "2         64410     86047875  AfricanAmerican  Female  [20-30)      ?   \n",
       "3        500364     82442376        Caucasian    Male  [30-40)      ?   \n",
       "4         16680     42519267        Caucasian    Male  [40-50)      ?   \n",
       "\n",
       "   admission_type_id  discharge_disposition_id  admission_source_id  \\\n",
       "0                  6                        25                    1   \n",
       "1                  1                         1                    7   \n",
       "2                  1                         1                    7   \n",
       "3                  1                         1                    7   \n",
       "4                  1                         1                    7   \n",
       "\n",
       "   time_in_hospital  ... citoglipton insulin  glyburide-metformin  \\\n",
       "0                 1  ...          No      No                   No   \n",
       "1                 3  ...          No      Up                   No   \n",
       "2                 2  ...          No      No                   No   \n",
       "3                 2  ...          No      Up                   No   \n",
       "4                 1  ...          No  Steady                   No   \n",
       "\n",
       "   glipizide-metformin  glimepiride-pioglitazone  metformin-rosiglitazone  \\\n",
       "0                   No                        No                       No   \n",
       "1                   No                        No                       No   \n",
       "2                   No                        No                       No   \n",
       "3                   No                        No                       No   \n",
       "4                   No                        No                       No   \n",
       "\n",
       "   metformin-pioglitazone  change diabetesMed readmitted  \n",
       "0                      No      No          No         NO  \n",
       "1                      No      Ch         Yes        >30  \n",
       "2                      No      No         Yes         NO  \n",
       "3                      No      Ch         Yes         NO  \n",
       "4                      No      Ch         Yes         NO  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the dataset\n",
    "path = \"D:/Datasets/Diabetes/diabetic_data.csv\"\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "encounter_id                 int64\n",
       "patient_nbr                  int64\n",
       "race                        object\n",
       "gender                      object\n",
       "age                         object\n",
       "weight                      object\n",
       "admission_type_id            int64\n",
       "discharge_disposition_id     int64\n",
       "admission_source_id          int64\n",
       "time_in_hospital             int64\n",
       "payer_code                  object\n",
       "medical_specialty           object\n",
       "num_lab_procedures           int64\n",
       "num_procedures               int64\n",
       "num_medications              int64\n",
       "number_outpatient            int64\n",
       "number_emergency             int64\n",
       "number_inpatient             int64\n",
       "diag_1                      object\n",
       "diag_2                      object\n",
       "diag_3                      object\n",
       "number_diagnoses             int64\n",
       "max_glu_serum               object\n",
       "A1Cresult                   object\n",
       "metformin                   object\n",
       "repaglinide                 object\n",
       "nateglinide                 object\n",
       "chlorpropamide              object\n",
       "glimepiride                 object\n",
       "acetohexamide               object\n",
       "glipizide                   object\n",
       "glyburide                   object\n",
       "tolbutamide                 object\n",
       "pioglitazone                object\n",
       "rosiglitazone               object\n",
       "acarbose                    object\n",
       "miglitol                    object\n",
       "troglitazone                object\n",
       "tolazamide                  object\n",
       "examide                     object\n",
       "citoglipton                 object\n",
       "insulin                     object\n",
       "glyburide-metformin         object\n",
       "glipizide-metformin         object\n",
       "glimepiride-pioglitazone    object\n",
       "metformin-rosiglitazone     object\n",
       "metformin-pioglitazone      object\n",
       "change                      object\n",
       "diabetesMed                 object\n",
       "readmitted                  object\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check data types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Value transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['race', 'gender', 'age', 'weight', 'admission_type_id',\n",
       "       'discharge_disposition_id', 'admission_source_id', 'time_in_hospital',\n",
       "       'medical_specialty', 'num_lab_procedures', 'num_procedures',\n",
       "       'num_medications', 'number_outpatient', 'number_emergency',\n",
       "       'number_inpatient', 'diag_1', 'diag_2', 'diag_3', 'number_diagnoses',\n",
       "       'max_glu_serum', 'A1Cresult', 'metformin', 'repaglinide', 'nateglinide',\n",
       "       'chlorpropamide', 'glimepiride', 'glipizide', 'glyburide',\n",
       "       'tolbutamide', 'pioglitazone', 'rosiglitazone', 'acarbose', 'miglitol',\n",
       "       'troglitazone', 'tolazamide', 'insulin', 'glyburide-metformin',\n",
       "       'glipizide-metformin', 'glimepiride-pioglitazone',\n",
       "       'metformin-rosiglitazone', 'metformin-pioglitazone', 'change',\n",
       "       'diabetesMed', 'readmitted'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop acetohexamide, cytoglipton and examide columns which have only 1 value\n",
    "#Drop encounter_id, patient_nbr and payer_code which are meaningless\n",
    "df.drop([\"acetohexamide\", \"citoglipton\", \"examide\", \"encounter_id\", \"patient_nbr\", \"payer_code\"], axis=1, inplace=True)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None      84748\n",
       "Abnorm    12028\n",
       "Norm       4990\n",
       "Name: A1Cresult, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For AICresult column\n",
    "def Replace_values_A1C(a):\n",
    "    if a == \">7\" or a == \">8\":\n",
    "        return \"Abnorm\"\n",
    "    elif a == \"?\":\n",
    "        return np.NaN\n",
    "    else:\n",
    "        return a\n",
    "\n",
    "df[\"A1Cresult\"] = df[\"A1Cresult\"].map(lambda x: Replace_values_A1C(x))\n",
    "df[\"A1Cresult\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None      96420\n",
       "Abnorm     2749\n",
       "Norm       2597\n",
       "Name: max_glu_serum, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For max_glu_serum column\n",
    "def Replace_values_MGS(a):\n",
    "    if a == \">200\" or a == \">300\":\n",
    "        return \"Abnorm\"\n",
    "    elif a == \"?\":\n",
    "        return np.NaN\n",
    "    else:\n",
    "        return a\n",
    "    \n",
    "df[\"max_glu_serum\"] = df[\"max_glu_serum\"].map(lambda x: Replace_values_MGS(x))\n",
    "df[\"max_glu_serum\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "elderly    68541\n",
       "adult      32373\n",
       "young        691\n",
       "child        161\n",
       "Name: age, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For age column\n",
    "def Replace_values_Age(a):\n",
    "    if a == \"[0-10)\":\n",
    "        return \"child\"\n",
    "    elif a == \"[10-20)\":\n",
    "        return \"young\"\n",
    "    elif a == \"[20-30)\" or a == \"[30-40)\" or a == \"[40-50)\" or a == \"[50-60)\":\n",
    "        return \"adult\"\n",
    "    elif a == \"?\":\n",
    "        return np.NaN\n",
    "    else:\n",
    "        return \"elderly\"\n",
    "    \n",
    "df[\"age\"] = df[\"age\"].map(lambda x: Replace_values_Age(x))\n",
    "df[\"age\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n",
       "        11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,\n",
       "        22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.,  32.,\n",
       "        33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.,  41.,  42.,  43.,\n",
       "        44.,  45.,  46.,  47.,  48.,  49.,  50.,  51.,  52.,  53.,  54.,\n",
       "        55.,  56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,\n",
       "        66.,  67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,\n",
       "        77.,  78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,\n",
       "        88.,  89.,  90.,  91.,  92.,  93.,  94.,  95.,  96.,  97.,  98.,\n",
       "        99., 100., 101., 102., 103., 104., 105., 106., 107., 108., 109.,\n",
       "       110., 111., 112., 113., 114., 115., 116., 117., 118., 119., 120.,\n",
       "       121., 122., 123., 124., 125., 126., 127., 128., 129., 130., 131.,\n",
       "       132., 133., 134., 135., 136., 137., 138.,  nan, 139., 140., 141.,\n",
       "       142., 143., 144., 145., 146., 147., 148., 149., 150., 151., 152.,\n",
       "       153., 154., 155., 156., 157., 158., 159., 160., 161., 162., 163.,\n",
       "       164., 165., 166., 167., 168., 169., 170., 171., 172., 173., 174.,\n",
       "       175., 176., 177., 178., 179., 180., 181., 182., 183., 184., 185.,\n",
       "       186., 187., 188., 189., 190., 191., 192., 193., 194., 195., 196.,\n",
       "       197., 198., 199., 200., 201., 202., 203., 204., 205., 206., 207.,\n",
       "       208., 209., 210., 211., 212., 213., 214., 215., 216., 217., 218.,\n",
       "       219., 220., 221., 222., 223., 224., 225., 226., 227., 228., 229.,\n",
       "       230., 231., 232., 233., 234., 235., 236., 237., 238., 239., 240.,\n",
       "       241., 242., 243., 244., 245., 246., 247., 248., 249., 250., 251.,\n",
       "       252., 253., 254., 255., 256., 257., 258., 259., 260., 261., 262.,\n",
       "       263., 264., 265., 266., 267., 268., 269., 270., 271., 272., 273.,\n",
       "       274., 275., 276., 277., 278., 279., 280., 281., 282., 283., 284.,\n",
       "       285., 286., 287., 288., 289., 290., 291., 292., 293., 294., 295.,\n",
       "       296., 297., 298., 299., 300., 301., 302., 303., 304., 305., 306.,\n",
       "       307., 308., 309., 310., 311., 312., 313., 314., 315., 316., 317.,\n",
       "       318., 319., 320., 321., 322., 323., 324., 325., 326., 327., 328.,\n",
       "       329., 330., 331., 332., 333., 334., 335., 336., 337., 338., 339.,\n",
       "       340., 341., 342., 343., 344., 345., 346., 347., 348., 349., 350.,\n",
       "       351., 352., 353., 354., 355., 356., 357., 358., 359., 360., 361.,\n",
       "       362., 363., 364., 365., 366., 367., 368., 369., 370., 371., 372.,\n",
       "       373., 374., 375., 376., 377., 378., 379., 380., 381., 382., 383.,\n",
       "       384., 385., 386., 387., 388., 389., 390., 391., 392., 393., 394.,\n",
       "       395., 396., 397., 398., 399., 400., 401., 402., 403., 404., 405.,\n",
       "       406., 407., 408., 409., 410., 411., 412., 413., 414., 415., 416.,\n",
       "       417., 418., 419., 420., 421., 422., 423., 424., 425., 426., 427.,\n",
       "       428., 429., 430., 431., 432., 433., 434., 435., 436., 437., 438.,\n",
       "       439., 440., 441., 442., 443., 444., 445., 446., 447., 448., 449.,\n",
       "       450., 451., 452., 453., 454., 455., 456., 457., 458., 459., 460.,\n",
       "       461., 462., 463., 464., 465., 466., 467., 468., 469., 470., 471.,\n",
       "       472., 473., 474., 475., 476., 477., 478., 479., 480., 481., 482.,\n",
       "       483., 484., 485., 486., 487., 488., 489., 490., 491., 492., 493.,\n",
       "       494., 495., 496., 497., 498., 499., 500., 501., 502., 503., 504.,\n",
       "       505., 506., 507., 508., 509., 510., 511., 512., 513., 514., 515.,\n",
       "       516., 517., 518., 519., 520., 521., 522., 523., 524., 525., 526.,\n",
       "       527., 528., 529., 530., 531., 532., 533., 534., 535., 536., 537.,\n",
       "       538., 539., 540., 541., 542., 543., 544., 545., 546., 547., 548.,\n",
       "       549., 550., 551., 552., 553., 554., 555., 556., 557., 558., 559.,\n",
       "       560., 561., 562., 563., 564., 565., 566., 567., 568., 569., 570.,\n",
       "       571., 572., 573., 574., 575., 576., 577., 578., 579., 580., 581.,\n",
       "       582., 583., 584., 585., 586., 587., 588., 589., 590., 591., 592.,\n",
       "       593., 594., 595., 596., 597., 598., 599., 600., 601., 602., 603.,\n",
       "       604., 605., 606., 607., 608., 609., 610., 611., 612., 613., 614.,\n",
       "       615., 616., 617., 618., 619., 620., 621., 622., 623., 624., 625.,\n",
       "       626., 627., 628., 629., 630., 631., 632., 633., 634., 635., 636.,\n",
       "       637., 638., 639., 640., 641., 642., 643., 644., 645., 646., 647.,\n",
       "       648., 649., 650., 651., 652., 653., 654., 655., 656., 657., 658.,\n",
       "       659., 660., 661., 662., 663., 664., 665., 666., 667., 668., 669.,\n",
       "       670., 671., 672., 673., 674., 675., 676., 677., 678., 679., 680.,\n",
       "       681., 682., 683., 684., 685., 686., 687., 688., 689., 690., 691.,\n",
       "       692., 693., 694., 695., 696., 697., 698., 699., 700., 701., 702.,\n",
       "       703., 704., 705., 706., 707., 708., 709., 710., 711., 712., 713.,\n",
       "       714., 715.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For diag_1, diag_2 and diag_3 columns\n",
    "#Map the elements with the unique IDs\n",
    "values_d1 = df[\"diag_1\"].unique().tolist()\n",
    "values_d1.remove(\"?\")\n",
    "IDs_d1 = np.arange(len(values_d1))\n",
    "map_d1 = dict(zip(values_d1, IDs_d1))\n",
    "map_d1[\"?\"] = np.NaN\n",
    "\n",
    "values_d2 = df[\"diag_2\"].unique().tolist()\n",
    "values_d2.remove(\"?\")\n",
    "IDs_d2 = np.arange(len(values_d2))\n",
    "map_d2 = dict(zip(values_d2, IDs_d2))\n",
    "map_d2[\"?\"] = np.NaN\n",
    "\n",
    "values_d3 = df[\"diag_3\"].unique().tolist()\n",
    "values_d3.remove(\"?\")\n",
    "IDs_d3 = np.arange(len(values_d3))\n",
    "map_d3 = dict(zip(values_d3, IDs_d3))\n",
    "map_d3[\"?\"] = np.NaN\n",
    "\n",
    "df[\"diag_1\"] = df[\"diag_1\"].map(map_d1)\n",
    "df[\"diag_2\"] = df[\"diag_2\"].map(map_d2)\n",
    "df[\"diag_3\"] = df[\"diag_3\"].map(map_d3)\n",
    "\n",
    "df[\"diag_1\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    90409\n",
       "1    11357\n",
       "Name: readmitted, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For readmitted column\n",
    "#Readmitted within 30 days -> 1,  not -> 0\n",
    "def Replace_values_Readmitted(a):\n",
    "    if a == \"<30\":\n",
    "        return 1\n",
    "    elif a == \"?\":\n",
    "        return np.NaN\n",
    "    else: return 0\n",
    "    \n",
    "df[\"readmitted\"] = df[\"readmitted\"].map(lambda x: Replace_values_Readmitted(x))\n",
    "df[\"readmitted\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Map other nominal features\n",
    "def Map_Object_Column(column):\n",
    "    values = column.unique().tolist()\n",
    "    \n",
    "    if \"?\" in values:\n",
    "        values.remove(\"?\")\n",
    "    else: pass\n",
    "    \n",
    "    IDs = np.arange(len(values))\n",
    "    map_column = dict(zip(values, IDs))\n",
    "    map_column[\"?\"] = np.NaN\n",
    "    \n",
    "    return column.map(map_column)\n",
    "\n",
    "for column in df.columns:\n",
    "    if df[column].dtype == \"object\":\n",
    "        df[column] = Map_Object_Column(df[column])\n",
    "    else: pass\n",
    "    \n",
    "\n",
    "#Restore the dataset after value transformation\n",
    "df.to_csv(\"D:/Datasets/Diabetes/Diabetes_withMissingValues.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Missing value handling and data balancing\n",
    "### 2.1 Drop the columns with missing value > 40%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Find the missing values\n",
    "def Missing_Value_Identification(a):\n",
    "    if a == \"?\":\n",
    "        return np.NaN\n",
    "    else: return a\n",
    "\n",
    "for column in df.columns:\n",
    "    df[column] = df[column].map(lambda x: Missing_Value_Identification(x))\n",
    "\n",
    "#Proportion of missing values\n",
    "df.isnull().sum()/df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Drop the columns with missing% > 40%\n",
    "df.drop([\"weight\", \"medical_specialty\"], axis=1, inplace=True)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the index of the missing values\n",
    "    #Method for identifying the nan values in a given row\n",
    "idex_nans = dict()\n",
    "\n",
    "def Index_Nan_In_Row(row_id):\n",
    "    row = df.iloc[row_id,:]\n",
    "    idx = np.arange(len(row))\n",
    "    idx_nan = [i for i in idx if str(row[i])==\"nan\"]\n",
    "    \n",
    "    if idx_nan != []:\n",
    "        idex_nans[str(row_id)]=str(idx_nan)\n",
    "    else: pass\n",
    "\n",
    "#Loop all the rows\n",
    "for row_id in range(df.shape[0]):\n",
    "    Index_Nan_In_Row(row_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the missing values info\n",
    "import json\n",
    "\n",
    "with open(\"D:/Datasets/Diabetes/missing_value_idx.json\", \"w\") as fp:\n",
    "    json.dump(idex_nans, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Arithmetic mean and mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Since the rest columns with missing values are of nominal type\n",
    "Use mode to fill the missing data'''\n",
    "\n",
    "#Check the nominal features\n",
    "df_mode = df.copy()\n",
    "mode_race = df_mode[\"race\"].value_counts(sort=True,ascending=False)[0]\n",
    "mode_d1 = df_mode[\"diag_1\"].value_counts(sort=True,ascending=False)[0]\n",
    "mode_d2 = df_mode[\"diag_2\"].value_counts(sort=True,ascending=False)[0]\n",
    "mode_d3 = df_mode[\"diag_3\"].value_counts(sort=True,ascending=False)[0]\n",
    "\n",
    "mode_race, mode_d1, mode_d2, mode_d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mode[\"race\"].fillna(mode_race, inplace=True)\n",
    "df_mode[\"diag_1\"].fillna(mode_d1, inplace=True)\n",
    "df_mode[\"diag_2\"].fillna(mode_d2, inplace=True)\n",
    "df_mode[\"diag_3\"].fillna(mode_d3, inplace=True)\n",
    "\n",
    "df_mode.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 SMOTE and undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the predictors and target\n",
    "\n",
    "X = df_mode.iloc[:, :-1]\n",
    "Y = df_mode.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMOTE the minority class, undersample the majority class\n",
    "smote = SMOTE(n_jobs=-1)\n",
    "enn = EditedNearestNeighbours(n_jobs=-1)\n",
    "smoteenn = SMOTEENN(smote=smote, enn=enn, random_state=100)\n",
    "\n",
    "X_resampled, Y_resampled = smoteenn.fit_resample(X, Y)\n",
    "\n",
    "Y_resampled.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Random Forest method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Tuning of RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #1. Pre-train the RF\n",
    "# #Tune with Hyperparameter range\n",
    "# param_grid={\n",
    "#     \"n_estimators\":[800,900,1000],\n",
    "#     \"max_depth\":[3,4,5,6],\n",
    "#     \"min_impurity_decrease\":[0,0.025,0.05],\n",
    "#     \"max_samples\":[15, 20, 30]\n",
    "# }\n",
    "\n",
    "# rf_test = RandomForestClassifier(criterion=\"gini\", bootstrap=True, oob_score=True, n_jobs=-1, random_state=1)\n",
    "# search = GridSearchCV(rf_test, param_grid, n_jobs=-1)\n",
    "# search.fit(X_resampled, Y_resampled)\n",
    "\n",
    "# print(\"Best parameter (ob score=%0.3f):\" % search.best_score_)\n",
    "# print(search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Pretrain a RF with the best hypa\n",
    "rf = RandomForestClassifier(n_estimators=1000, criterion=\"gini\", \\\n",
    "                                    max_depth=5, min_impurity_decrease=0., max_samples=30,\\\n",
    "                            bootstrap=True, oob_score=True, n_jobs=-1, random_state=1)\n",
    "rf.fit(X_resampled, Y_resampled)\n",
    "\n",
    "print(\"OOB acc: \", rf.oob_score_)\n",
    "\n",
    "#Save the trained RF\n",
    "joblib.dump(rf, \"D:/RF_pretrained.sav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the dataset processed by RF missing value method in the last iteration\n",
    "df_rf1=pd.read_csv(\"D:/Diabetes_FillMissing_by_RF1.csv\")\n",
    "\n",
    "df_rf1 = df_rf1.iloc[:,1:]\n",
    "df_rf1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Apply the RF on the original dataset X which has the initial guesses\n",
    "rf = joblib.load(\"D:/RF_pretrained.sav\")\n",
    "leaf_idx = rf.apply(df_rf1.iloc[:,:-1])\n",
    "\n",
    "np.unique(leaf_idx, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Proximity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Build proximity matrix\n",
    "import time\n",
    "from numba import jit,guvectorize, int64, vectorize, int32\n",
    "from numba import cuda\n",
    "\n",
    "@jit(parallel=True)\n",
    "def Comparison(arr1, arr2):\n",
    "    comparison = arr1 == arr2\n",
    "    return np.sum(comparison)\n",
    "    \n",
    "    \n",
    "def Proximity_Matrix(pm, leaf_idx):\n",
    "#     global pm\n",
    "    n_samples=leaf_idx.shape[0]\n",
    "    \n",
    "    #Update the matrix\n",
    "    start=time.time()\n",
    "    for i in range(n_samples-1):\n",
    "        for j in range(i+1, n_samples):\n",
    "\n",
    "            #Compare the similarity between the samples in the leaf index\n",
    "            adds = Comparison(leaf_idx[i], leaf_idx[j])\n",
    "\n",
    "            pm[i][j] += adds\n",
    "            pm[j][i] += adds\n",
    "            \n",
    "        #Demonstrate the progress    \n",
    "        if i%200==0:\n",
    "            end=time.time()\n",
    "            \n",
    "            n_total = n_samples*(n_samples-1)/2\n",
    "            n_elapsed = (n_samples-1+n_samples-1-(i+1))*(i+1)/2\n",
    "            \n",
    "            time_elapsed = (end-start)//60\n",
    "            print(\"PM progress... {}-th 200-sample is done! Elapsed time {} mins\\n\\\n",
    "            Rest time: {} mins\".format((i+1)//200, time_elapsed, (time_elapsed/n_elapsed)*n_total//60+1))\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pm = np.zeros(shape=(leaf_idx.shape[0], leaf_idx.shape[0]), dtype=np.int16)\n",
    "Proximity_Matrix(pm, leaf_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3,4,5,4,7,8,4])\n",
    "idx = np.argwhere(a==4)\n",
    "a[idx].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"D:/pm2.npy\", pm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the value frequency in DF\n",
    "@jit\n",
    "def Get_Value_Frequency(value, arr):\n",
    "    #Find the number of the desired values in the column\n",
    "    idx = np.argwhere(arr==value)\n",
    "    \n",
    "    #The sum of the desired value counts\n",
    "    value_sum = arr[idx].size\n",
    "    \n",
    "    return value_sum/arr.size\n",
    "\n",
    "\n",
    "#Caluculate weight frequency\n",
    "@jit(parallel=True)\n",
    "def Weight_Frequency(pm, row_id, P_value, F_value):\n",
    "    #Proximity of all the values\n",
    "    P_values = pm[row_id].sum()\n",
    "    W_value = P_value/P_values\n",
    "\n",
    "    #Weight Frequency of the value\n",
    "    return F_value*W_value\n",
    "\n",
    "\n",
    "#4. Calculate the final guess\n",
    "\n",
    "def Refine_Guess(row_id, column_id, df, pm, nominal):\n",
    "    #column_id: the column needed to evaluate\n",
    "    #row_id: the sample which have missing values in the column\n",
    "    #df: the original dataset with missing values\n",
    "    #pm:the proximity matrix\n",
    "    \n",
    "    #All the unique values in the original column\n",
    "    arr_unique = np.unique(df[:,column_id])\n",
    "    values = arr_unique[~np.isnan(arr_unique)]\n",
    "    \n",
    "    #For nominal feature, use weight frequency\n",
    "    #For numeric feature, use weight average of the values\n",
    "    WF_values=[]\n",
    "    VF_values=0\n",
    "    for value in tqdm(values):\n",
    "        #Frequency of the value in the column\n",
    "        F_value = Get_Value_Frequency(value=value, arr=df[:,column_id])\n",
    "    \n",
    "        #Weight in proximity matrix\n",
    "            #Proximity of the value\n",
    "        P_value=0\n",
    "            \n",
    "        for v in range(len(df[:,column_id])):\n",
    "            if df[:,column_id][v] == value:\n",
    "                #Get the proximity from PM by row_id and column id\n",
    "                P_value+=pm[row_id][v]\n",
    "            \n",
    "            #Proximity of all the values\n",
    "        WF_values.append(Weight_Frequency(pm, row_id, P_value, F_value))\n",
    "        \n",
    "        #Weight Average of the value\n",
    "#         VF_values+=value*W_value\n",
    "\n",
    "    #Find the highest WF_value's index for nominal feature\n",
    "    idx = np.argmax(WF_values)\n",
    "    \n",
    "    #Return the final guessed value\n",
    "    if nominal==True:\n",
    "        return values[idx]\n",
    "#     else:\n",
    "#         return VF_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Repeat 1~ 4 steps\n",
    "#Unitl convergence between current guesses and the last guesses\n",
    "#or difference below some tolerance\n",
    "\n",
    "def Reat_Until_Converge(df, pm, max_iters, toler):\n",
    "    for itera in range(max_iters):\n",
    "#         x=df.iloc[:,:-1]\n",
    "#         y=df.iloc[:,-1]\n",
    "\n",
    "#         leaf_idx = Build_Apply_RF(x, y)\n",
    "#         print(\"Apply RF done... leaf_idx\", leaf_idx.shape)\n",
    "\n",
    "#         pm = Proximity_Matrix(leaf_idx)\n",
    "#         print(\"Proximity_Matrix built... \", pm.shape)\n",
    "        \n",
    "        #Define a tempo array for saving the values of the dataframe with initial guesses\n",
    "        df_rf = df.copy().to_numpy()\n",
    "\n",
    "        values_last_guessed = []\n",
    "        values_current_guess = []\n",
    "\n",
    "        for row_id in tqdm(list(idex_nans.keys())):\n",
    "            for column_id in idex_nans[int(row_id)]:\n",
    "                #Get the last guess\n",
    "                value_last_guessed = df_rf[int(row_id), column_id]\n",
    "                values_last_guessed.append(value_last_guessed)\n",
    "\n",
    "                #Get the next guess\n",
    "                value_current_guess = Refine_Guess(int(row_id), column_id, df_rf, pm, nominal=True)\n",
    "                values_current_guess.append(value_current_guess)\n",
    "\n",
    "                #Change the dataset according to current guess\n",
    "                df_rf[int(row_id), column_id] = value_current_guess\n",
    "\n",
    "        values_last_guessed=np.array(values_last_guessed)\n",
    "        values_current_guess=np.array(values_current_guess)\n",
    "        \n",
    "\n",
    "        error = np.sum(np.abs(values_last_guessed-values_current_guess))/values_last_guessed.shape[0]\n",
    "        print(\"Iteration: \", itera, \"  error: \", error)\n",
    "               \n",
    "        if error <= toler:\n",
    "            return df_rf, pm, error\n",
    "        \n",
    "    #Return pm, df\n",
    "    return df_rf, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_rf,error = Reat_Until_Converge(df_rf1, pm, 1, toler=0)\n",
    "df_rf.shape, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the dataset with missing value filled by RF\n",
    "dataframe_rf = pd.DataFrame(df_rf, columns=df_mode.columns)\n",
    "dataframe_rf.to_csv(\"D:/Diabetes_FillMissing_by_RF2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
